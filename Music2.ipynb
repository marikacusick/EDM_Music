{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "PATH = '/Users/marika/Documents/Github/EDM_Music'\n",
    "sys.path.append(os.path.join(PATH, 'midi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "\n",
    "import pretty_midi\n",
    "from midi_utils import midiread, midiwrite\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage.io as io\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "pth = Path('/Users/marika/Documents/Github/EDM_Music/Lead-Sheet-Dataset'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "def midi_filename_to_piano_roll(midi_filename):\n",
    "    \n",
    "    midi_data = midiread(midi_filename, dt=0.3)\n",
    "    \n",
    "    piano_roll = midi_data.piano_roll.T\n",
    "    \n",
    "    # Binarize the pressed notes \n",
    "    piano_roll[piano_roll > 0] = 1  # no need as unique values are 0 and 1\n",
    "    \n",
    "    return piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_piano_roll(piano_roll, max_length=132333, pad_value=0):\n",
    "    \"\"\"\n",
    "    padding 0 at the beginning of sequence\n",
    "    \n",
    "    \"\"\"\n",
    "    # We hardcode 88 -- because we will always use only\n",
    "    # 88 pitches\n",
    "    \n",
    "    original_piano_roll_length = piano_roll.shape[1]\n",
    "    \n",
    "    padded_piano_roll = np.zeros((88, max_length))\n",
    "    padded_piano_roll[:] = pad_value\n",
    "    \n",
    "    padded_piano_roll[:, :original_piano_roll_length] = piano_roll  # keep 0 padding at begin\n",
    "\n",
    "    return padded_piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "midi_folders = os.listdir('./Lead-Sheet-Dataset/datasets/pianoroll/a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mLead-Sheet-Dataset\u001b[m\u001b[m/      dur_list_all_c.npy       octave2_prev_X_te.npy\r\n",
      "\u001b[34mMIDI_Files\u001b[m\u001b[m/              \u001b[34mmidi\u001b[m\u001b[m/                    octave2_prev_X_tr.npy\r\n",
      "\u001b[34mMidiNet-by-pytorch\u001b[m\u001b[m/      \u001b[34mmusic-generation-master\u001b[m\u001b[m/ prev_x.npy\r\n",
      "Music2.ipynb             music_edm_new.pth        project helper.ipynb\r\n",
      "\u001b[34mNottingham\u001b[m\u001b[m/              \u001b[34mmusic_venv\u001b[m\u001b[m/              requirements.txt\r\n",
      "Nottingham.pickle        note_list_all_c.npy      sample.mid\r\n",
      "README.md                octave2_X_te.npy\r\n",
      "data_x.npy               octave2_X_tr.npy\r\n"
     ]
    }
   ],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "midi_filenames = []\n",
    "base_dir = 'MIDI_Files/songs/'\n",
    "for i in midi_folders: \n",
    "    if i!='.DS_Store':\n",
    "        artist_folder = os.listdir('./Lead-Sheet-Dataset/datasets/pianoroll/a/' + str(i) + '/')\n",
    "        for j in artist_folder: \n",
    "            if j!='.DS_Store':\n",
    "                folder = os.listdir('./Lead-Sheet-Dataset/datasets/pianoroll/a/' + str(i) + '/' + str(j))\n",
    "                for f in folder:\n",
    "                    if '.mid' in f and 'nokey' not in f:\n",
    "                        source = './Lead-Sheet-Dataset/datasets/pianoroll/a/'+ str(i) + '/' + str(j) + \"/\"+ str(f) \n",
    "                        \n",
    "                        destination = os.path.join(base_dir, str(i) + '_' + str(j) + '_' + f)\n",
    "                        \n",
    "                        shutil.copyfile(source, destination)\n",
    "                    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_folder_path = './MIDI_Files/songs/'\n",
    "midi_filenames = os.listdir(midi_folder_path)\n",
    "midi_full_filenames = [os.path.join(midi_folder_path, filename) for filename in midi_filenames]\n",
    "sequences_lengths = [midi_filename_to_piano_roll(filename).shape[1] for filename in midi_full_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 228, 52.0, 64.77083333333333)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats of sequence lengths \n",
    "min(sequences_lengths), max(sequences_lengths), np.median(sequences_lengths), np.mean(sequences_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making validation slices of sequence length 50\n",
    "\n",
    "val_slices=[]\n",
    "\n",
    "for k in range(len(midi_filenames)):\n",
    "    piano_roll = midi_filename_to_piano_roll(midi_full_filenames[k])\n",
    "    \n",
    "    for i in range(piano_roll.shape[1]):\n",
    "        if i%50 == 0:\n",
    "            #print ('here')\n",
    "            try:\n",
    "                val_slices.append(piano_roll[:,i:i+51])\n",
    "            except IndexError:\n",
    "                pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_list = [v for v in val_slices if v.shape[1] == 51]\n",
    "len(validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, midi_folder_path, longest_sequence_length=50):\n",
    "        \n",
    "        self.midi_folder_path = midi_folder_path\n",
    "        \n",
    "        midi_filenames = os.listdir(midi_folder_path)\n",
    "        \n",
    "        self.longest_sequence_length = longest_sequence_length\n",
    "        \n",
    "        midi_full_filenames = [os.path.join(midi_folder_path, filename) for filename in midi_filenames]\n",
    "        \n",
    "        self.midi_full_filenames = list(midi_full_filenames)\n",
    "        \n",
    "        if longest_sequence_length is None:            \n",
    "            self.update_the_max_length()\n",
    "    \n",
    "    \n",
    "    def update_the_max_length(self):\n",
    "        \"\"\"Recomputes the longest sequence constant of the dataset.\n",
    "\n",
    "        Reads all the midi files from the midi folder and finds the max\n",
    "        length.\n",
    "        \"\"\"\n",
    "        \n",
    "        sequences_lengths = [midi_filename_to_piano_roll(filename).shape[1] for filename in self.midi_full_filenames]\n",
    "        \n",
    "        max_length = max(sequences_lengths)\n",
    "        \n",
    "        self.longest_sequence_length = max_length\n",
    "                \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.midi_full_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        midi_full_filename = self.midi_full_filenames[index]\n",
    "        piano_roll = midi_filename_to_piano_roll(midi_full_filename)\n",
    "        \n",
    "        # -1 because we will shift it\n",
    "        sequence_length = piano_roll.shape[1] - 1\n",
    "        \n",
    "        # median length of songs\n",
    "        if piano_roll.shape[1] > self.longest_sequence_length:\n",
    "                random_strt_idx = np.random.randint(0, piano_roll.shape[1] -\n",
    "                                                    self.longest_sequence_length+3, 1) # +3 just to make sure of gaps at the end\n",
    "                ground_truth_sequence = piano_roll[:, \n",
    "                                            random_strt_idx[0] :\n",
    "                                            random_strt_idx[0]+self.longest_sequence_length]\n",
    "                input_sequence = piano_roll[:,random_strt_idx[0]-1 :\n",
    "                                            random_strt_idx[0]+self.longest_sequence_length-1]\n",
    "\n",
    "        else:\n",
    "            input_sequence = piano_roll[:,:-1]\n",
    "            ground_truth_sequence = piano_roll[:, 1:]        \n",
    "        \n",
    "        \n",
    "        # pad sequence so that all of them have the same lenght\n",
    "        # Otherwise the batching won't work\n",
    "        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length) \n",
    "        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,\n",
    "                                                      max_length=self.longest_sequence_length,\n",
    "                                                      pad_value=0)\n",
    "                \n",
    "        input_sequence_padded = input_sequence_padded.transpose()\n",
    "        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n",
    "        \n",
    "        return (torch.FloatTensor(input_sequence_padded),\n",
    "                torch.LongTensor(ground_truth_sequence_padded),\n",
    "                torch.LongTensor([self.longest_sequence_length]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset_validation(data.Dataset):\n",
    "    \n",
    "    def __init__(self, val_list, longest_sequence_length=50):\n",
    "        \n",
    "        self.val_list = val_list\n",
    "        self.longest_sequence_length = longest_sequence_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.val_list)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        piano_roll = self.val_list[index]\n",
    "        input_sequence = piano_roll[:,:-1]\n",
    "        ground_truth_sequence = piano_roll[:, 1:]        \n",
    "   \n",
    "        # pad sequence so that all of them have the same lenght\n",
    "        # Otherwise the batching won't work\n",
    "        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length) \n",
    "        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,\n",
    "                                                      max_length=self.longest_sequence_length,\n",
    "                                                      pad_value=0)\n",
    "                \n",
    "        input_sequence_padded = input_sequence_padded.transpose()\n",
    "        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n",
    "        \n",
    "        return (torch.FloatTensor(input_sequence_padded),\n",
    "                torch.LongTensor(ground_truth_sequence_padded),\n",
    "                torch.LongTensor([self.longest_sequence_length]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_process_sequence_batch(batch_tuple):\n",
    "    \n",
    "    input_sequences, output_sequences, lengths = batch_tuple\n",
    "    \n",
    "    splitted_input_sequence_batch = input_sequences.split(split_size=1) \n",
    "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
    "    splitted_lengths_batch = lengths.split(split_size=1)\n",
    "\n",
    "    training_data_tuples = zip(splitted_input_sequence_batch,\n",
    "                               splitted_output_sequence_batch,\n",
    "                               splitted_lengths_batch)\n",
    "\n",
    "    training_data_tuples_sorted = sorted(training_data_tuples,\n",
    "                                         key=lambda p: int(p[2]),\n",
    "                                         reverse=True)\n",
    "\n",
    "    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
    "\n",
    "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
    "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
    "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
    "    \n",
    "    # Here we trim overall data matrix using the size of the longest sequence\n",
    "    input_sequence_batch_sorted = input_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0], :]\n",
    "    output_sequence_batch_sorted = output_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0], :]\n",
    "    \n",
    "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
    "    \n",
    "    # pytorch's api for rnns wants lenghts to be list of ints\n",
    "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
    "    lengths_batch_sorted_list = map(lambda x: int(x), lengths_batch_sorted_list)\n",
    "    \n",
    "    return input_sequence_batch_transposed, output_sequence_batch_sorted, list(lengths_batch_sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = NotesGenerationDataset('./MIDI_Files/songs', longest_sequence_length=50)\n",
    "\n",
    "trainset_loader = data.DataLoader(trainset, batch_size=5,\n",
    "                                              shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(192, 38)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(trainset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valset = Dataset_validation(validation_list)\n",
    "\n",
    "valset_loader = data.DataLoader(valset, batch_size=5, shuffle=False, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout = 0.2)\n",
    "        \n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
    "        batch_size = input_sequences.shape[1]\n",
    "\n",
    "        notes_encoded = self.notes_encoder(input_sequences)\n",
    "        notes_encoded_rolled = notes_encoded.permute(1,2,0).contiguous()\n",
    "        notes_encoded_norm = self.bn(notes_encoded_rolled)\n",
    "        notes_encoded_norm_drop = nn.Dropout(0.25)(notes_encoded_norm)\n",
    "        notes_encoded_complete = notes_encoded_norm_drop.permute(2,0,1)\n",
    "        \n",
    "        # Here we run rnns only on non-padded regions of the batch\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(notes_encoded_complete, input_sequences_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        \n",
    "        outputs_norm = self.bn(outputs.permute(1,2,0).contiguous())\n",
    "        outputs_drop = nn.Dropout(0.1)(outputs_norm)\n",
    "        logits = self.logits_fc(outputs_drop.permute(2,0,1))\n",
    "        logits = logits.transpose(0, 1).contiguous()\n",
    "        \n",
    "        neg_logits = (1 - logits)\n",
    "        \n",
    "        # Since the BCE loss doesn't support masking, we use the crossentropy\n",
    "        binary_logits = torch.stack((logits, neg_logits), dim=3).contiguous()\n",
    "        logits_flatten = binary_logits.view(-1, 2)\n",
    "        return logits_flatten, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marika/Documents/GitHub/EDM_Music/music_venv/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_val = nn.CrossEntropyLoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = 1.0\n",
    "epochs_number = 10\n",
    "sample_history = []\n",
    "best_val_loss = float(\"inf\")\n",
    "layers = list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=88, out_features=512, bias=True),\n",
       " BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " LSTM(512, 512, num_layers=2, dropout=0.2),\n",
       " Linear(in_features=512, out_features=88, bias=True)]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrfinder(start, end, model, trainset_loader, epochs=20):\n",
    "    model.train() # into training mode\n",
    "    lrs = np.linspace(start, end, epochs*len(trainset_loader))    \n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters,start)\n",
    "    loss_list = []\n",
    "    ctr = 0\n",
    "    \n",
    "    for epoch_number in range(epochs):\n",
    "        epoch_loss = []\n",
    "        for batch in trainset_loader:\n",
    "            optimizer.param_groups[0]['lr'] = lrs[ctr]\n",
    "            ctr = ctr+1\n",
    "\n",
    "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1) )\n",
    "\n",
    "            input_sequences_batch_var = Variable( input_sequences_batch )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "            loss = criterion(logits, output_sequences_batch_var)\n",
    "            loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "        if epoch_number%5 == 0:\n",
    "            print('Epoch %d' % epoch_number)\n",
    "    plt.plot(lrs, loss_list)\n",
    "    return lrs, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model):\n",
    "    model.eval()\n",
    "    full_val_loss = 0.0\n",
    "    overall_sequence_length = 0.0\n",
    "\n",
    "    for batch in valset_loader:\n",
    "\n",
    "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "        output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1) )\n",
    "\n",
    "        input_sequences_batch_var = Variable( input_sequences_batch )\n",
    "\n",
    "        logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "        loss = criterion_val(logits, output_sequences_batch_var)\n",
    "\n",
    "        full_val_loss += loss.item()\n",
    "        overall_sequence_length += sum(sequences_lengths)\n",
    "\n",
    "    return full_val_loss / (overall_sequence_length * 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "rnn = model\n",
    "# lrs, losses = lrfinder(1e-4, 1e-1, rnn, trainset_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, lrs_triangular, epochs_number=10, wd=0.0, best_val_loss=float(\"inf\")):\n",
    "    loss_list = []\n",
    "    val_list =[]\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lrs_triangular[0], weight_decay=wd)\n",
    "    ctr = 0\n",
    "    \n",
    "    for epoch_number in range(epochs_number):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        for batch in trainset_loader:\n",
    "#             try: \n",
    "#                 optimizer.param_groups[0]['lr'] = lrs_triangular[ctr]\n",
    "#             except IndexError: pass\n",
    "#             ctr+=1\n",
    "\n",
    "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1) )\n",
    "\n",
    "            input_sequences_batch_var = Variable( input_sequences_batch )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "            loss = criterion(logits, output_sequences_batch_var)\n",
    "            loss_list.append(loss.item())\n",
    "            epoch_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        current_trn_epoch = sum(epoch_loss)/len(trainset_loader)\n",
    "        current_val_loss = validate(model)\n",
    "        \n",
    "#         if epoch_number%5 == 0:\n",
    "        print('Training Loss: Epoch:',epoch_number,':', current_trn_epoch)   \n",
    "#         if epoch_number%5 == 0:\n",
    "        print('Validation Loss: Epoch:',epoch_number,':', current_val_loss)\n",
    "        print('')\n",
    "\n",
    "        val_list.append(current_val_loss)\n",
    "\n",
    "        if current_val_loss < best_val_loss:\n",
    "\n",
    "            torch.save(model.state_dict(), 'music_edm_new.pth')\n",
    "            best_val_loss = current_val_loss\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: Epoch: 0 : 1.0885675220113051\n",
      "Validation Loss: Epoch: 0 : 0.8233547289299242\n",
      "\n",
      "Training Loss: Epoch: 1 : 0.31242578300206286\n",
      "Validation Loss: Epoch: 1 : 0.11325254374802715\n",
      "\n",
      "Training Loss: Epoch: 2 : 0.09421269968152046\n",
      "Validation Loss: Epoch: 2 : 0.08307043498132366\n",
      "\n",
      "Training Loss: Epoch: 3 : 0.07849626576429919\n",
      "Validation Loss: Epoch: 3 : 0.07791863417705702\n",
      "\n",
      "Training Loss: Epoch: 4 : 0.07259068128309752\n",
      "Validation Loss: Epoch: 4 : 0.06964785591921822\n",
      "\n",
      "Training Loss: Epoch: 5 : 0.07035328300767824\n",
      "Validation Loss: Epoch: 5 : 0.07159735744489162\n",
      "\n",
      "Training Loss: Epoch: 6 : 0.06739476764280546\n",
      "Validation Loss: Epoch: 6 : 0.0733328269675926\n",
      "\n",
      "Training Loss: Epoch: 7 : 0.06628650524898579\n",
      "Validation Loss: Epoch: 7 : 0.06987987849688289\n",
      "\n",
      "Training Loss: Epoch: 8 : 0.06724043190479279\n",
      "Validation Loss: Epoch: 8 : 0.06818442584207965\n",
      "\n",
      "Training Loss: Epoch: 9 : 0.06598181699059512\n",
      "Validation Loss: Epoch: 9 : 0.06772875812158038\n",
      "\n",
      "Training Loss: Epoch: 10 : 0.0637852230942563\n",
      "Validation Loss: Epoch: 10 : 0.0659574202720565\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "best_val_loss = train_model(rnn, [1e-3], epochs_number=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(sample_length=4, temperature=1, starting_sequence=None):\n",
    "\n",
    "    if starting_sequence is None:\n",
    "                \n",
    "        current_sequence_input = torch.zeros(1, 1, 88)\n",
    "        current_sequence_input[0, 0, 40] = 1\n",
    "        current_sequence_input[0, 0, 50] = 0\n",
    "        current_sequence_input[0, 0, 56] = 0\n",
    "        current_sequence_input = Variable(current_sequence_input)\n",
    "    \n",
    "    else:\n",
    "        current_sequence_input = Variable(starting_sequence)\n",
    "    final_output_sequence = [current_sequence_input.data.squeeze(1)]\n",
    "    \n",
    "    hidden = None    \n",
    "\n",
    "    for i in range(sample_length):\n",
    "\n",
    "        output, hidden = rnn(current_sequence_input, [1], hidden)\n",
    "\n",
    "        probabilities = nn.functional.softmax(output.div(temperature), dim=1)\n",
    "\n",
    "        current_sequence_input = torch.multinomial(probabilities.data, 1).squeeze().unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        current_sequence_input = Variable(current_sequence_input.float())\n",
    "\n",
    "        final_output_sequence.append(current_sequence_input.data.squeeze(1))\n",
    "\n",
    "    sampled_sequence = torch.cat(final_output_sequence, dim=0).numpy()\n",
    "    \n",
    "    return sampled_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x107fef080>"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = sample_from_piano_rnn(sample_length=80, temperature=0.8).transpose()\n",
    "io.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "midiwrite('sample.mid', sample.transpose(), dt=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='sample.mid' target='_blank'>sample.mid</a><br>"
      ],
      "text/plain": [
       "/Users/marika/Documents/GitHub/EDM_Music/sample.mid"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('sample.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_venv",
   "language": "python",
   "name": "music_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
