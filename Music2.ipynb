{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "PATH = '/Users/marika/Documents/Github/EDM_Music'\n",
    "sys.path.append(os.path.join(PATH, 'midi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "\n",
    "import pretty_midi\n",
    "from midi_utils import midiread, midiwrite\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage.io as io\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "pth = Path('/Users/marika/Documents/Github/EDM_Music/Lead-Sheet-Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "def midi_filename_to_piano_roll(midi_filename):\n",
    "    \n",
    "    midi_data = midiread(midi_filename, dt=0.3)\n",
    "    \n",
    "    piano_roll = midi_data.piano_roll.T\n",
    "    \n",
    "    # Binarize the pressed notes \n",
    "    piano_roll[piano_roll > 0] = 1  # no need as unique values are 0 and 1\n",
    "    \n",
    "    return piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_piano_roll(piano_roll, max_length=132333, pad_value=0):\n",
    "    \"\"\"\n",
    "    padding 0 at the beginning of sequence\n",
    "    \n",
    "    \"\"\"\n",
    "    # We hardcode 88 -- because we will always use only\n",
    "    # 88 pitches\n",
    "    \n",
    "    original_piano_roll_length = piano_roll.shape[1]\n",
    "    \n",
    "    padded_piano_roll = np.zeros((88, max_length))\n",
    "    padded_piano_roll[:] = pad_value\n",
    "    \n",
    "    padded_piano_roll[:, :original_piano_roll_length] = piano_roll  # keep 0 padding at begin\n",
    "\n",
    "    return padded_piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "midi_folders = os.listdir('./Lead-Sheet-Dataset/datasets/pianoroll/m')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in-and-out-of-love']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "artist_folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "midi_filenames = []\n",
    "base_dir = 'MIDI_Files/nokey'\n",
    "for i in midi_folders: \n",
    "    if i!='.DS_Store':\n",
    "        artist_folder = os.listdir('./Lead-Sheet-Dataset/datasets/pianoroll/m/' + str(i) + '/')\n",
    "        for j in artist_folder: \n",
    "            if j!='.DS_Store':\n",
    "                folder = os.listdir('./Lead-Sheet-Dataset/datasets/pianoroll/m/' + str(i) + '/' + str(j))\n",
    "                for f in folder:\n",
    "                    if '.mid' in f and 'nokey' in f:\n",
    "                        source = './Lead-Sheet-Dataset/datasets/pianoroll/m/'+ str(i) + '/' + str(j) + \"/\"+ str(f) \n",
    "                        \n",
    "                        destination = os.path.join(base_dir, str(i) + '_' + str(j) + '_' + f)\n",
    "                        \n",
    "                        shutil.copyfile(source, destination)\n",
    "                    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.remove('./MIDI_Files/songs/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_folder_path = './MIDI_Files/songs/'\n",
    "midi_filenames = os.listdir(midi_folder_path)\n",
    "midi_full_filenames = [os.path.join(midi_folder_path, filename) for filename in midi_filenames]\n",
    "sequences_lengths = [midi_filename_to_piano_roll(filename).shape[1] for filename in midi_full_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13, 228, 52.0, 65.56223175965665)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats of sequence lengths \n",
    "min(sequences_lengths), max(sequences_lengths), np.median(sequences_lengths), np.mean(sequences_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# making validation slices of sequence length 50\n",
    "\n",
    "val_slices=[]\n",
    "\n",
    "for k in range(len(midi_filenames)):\n",
    "    piano_roll = midi_filename_to_piano_roll(midi_full_filenames[k])\n",
    "    \n",
    "    for i in range(piano_roll.shape[1]):\n",
    "        if i%50 == 0:\n",
    "            #print ('here')\n",
    "            try:\n",
    "                val_slices.append(piano_roll[:,i:i+51])\n",
    "            except IndexError:\n",
    "                pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_list = [v for v in val_slices if v.shape[1] == 51]\n",
    "len(validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, midi_folder_path, longest_sequence_length=50):\n",
    "        \n",
    "        self.midi_folder_path = midi_folder_path\n",
    "        \n",
    "        midi_filenames = os.listdir(midi_folder_path)\n",
    "        \n",
    "        self.longest_sequence_length = longest_sequence_length\n",
    "        \n",
    "        midi_full_filenames = [os.path.join(midi_folder_path, filename) for filename in midi_filenames]\n",
    "        \n",
    "        self.midi_full_filenames = list(midi_full_filenames)\n",
    "        \n",
    "        if longest_sequence_length is None:            \n",
    "            self.update_the_max_length()\n",
    "    \n",
    "    \n",
    "    def update_the_max_length(self):\n",
    "        \"\"\"Recomputes the longest sequence constant of the dataset.\n",
    "\n",
    "        Reads all the midi files from the midi folder and finds the max\n",
    "        length.\n",
    "        \"\"\"\n",
    "        \n",
    "        sequences_lengths = [midi_filename_to_piano_roll(filename).shape[1] for filename in self.midi_full_filenames]\n",
    "        \n",
    "        max_length = max(sequences_lengths)\n",
    "        \n",
    "        self.longest_sequence_length = max_length\n",
    "                \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.midi_full_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        midi_full_filename = self.midi_full_filenames[index]\n",
    "        piano_roll = midi_filename_to_piano_roll(midi_full_filename)\n",
    "        \n",
    "        # -1 because we will shift it\n",
    "        sequence_length = piano_roll.shape[1] - 1\n",
    "        \n",
    "        # median length of songs\n",
    "        if piano_roll.shape[1] > self.longest_sequence_length:\n",
    "                random_strt_idx = np.random.randint(0, piano_roll.shape[1] -\n",
    "                                                    self.longest_sequence_length+3, 1) # +3 just to make sure of gaps at the end\n",
    "                ground_truth_sequence = piano_roll[:, \n",
    "                                            random_strt_idx[0] :\n",
    "                                            random_strt_idx[0]+self.longest_sequence_length]\n",
    "                input_sequence = piano_roll[:,random_strt_idx[0]-1 :\n",
    "                                            random_strt_idx[0]+self.longest_sequence_length-1]\n",
    "\n",
    "        else:\n",
    "            input_sequence = piano_roll[:,:-1]\n",
    "            ground_truth_sequence = piano_roll[:, 1:]        \n",
    "        \n",
    "        \n",
    "        # pad sequence so that all of them have the same lenght\n",
    "        # Otherwise the batching won't work\n",
    "        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length) \n",
    "        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,\n",
    "                                                      max_length=self.longest_sequence_length,\n",
    "                                                      pad_value=0)\n",
    "                \n",
    "        input_sequence_padded = input_sequence_padded.transpose()\n",
    "        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n",
    "        \n",
    "        return (torch.FloatTensor(input_sequence_padded),\n",
    "                torch.LongTensor(ground_truth_sequence_padded),\n",
    "                torch.LongTensor([self.longest_sequence_length]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset_validation(data.Dataset):\n",
    "    \n",
    "    def __init__(self, val_list, longest_sequence_length=50):\n",
    "        \n",
    "        self.val_list = val_list\n",
    "        self.longest_sequence_length = longest_sequence_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.val_list)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        piano_roll = self.val_list[index]\n",
    "        input_sequence = piano_roll[:,:-1]\n",
    "        ground_truth_sequence = piano_roll[:, 1:]        \n",
    "   \n",
    "        # pad sequence so that all of them have the same lenght\n",
    "        # Otherwise the batching won't work\n",
    "        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length) \n",
    "        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,\n",
    "                                                      max_length=self.longest_sequence_length,\n",
    "                                                      pad_value=0)\n",
    "                \n",
    "        input_sequence_padded = input_sequence_padded.transpose()\n",
    "        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n",
    "        \n",
    "        return (torch.FloatTensor(input_sequence_padded),\n",
    "                torch.LongTensor(ground_truth_sequence_padded),\n",
    "                torch.LongTensor([self.longest_sequence_length]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_process_sequence_batch(batch_tuple):\n",
    "    \n",
    "    input_sequences, output_sequences, lengths = batch_tuple\n",
    "    \n",
    "    splitted_input_sequence_batch = input_sequences.split(split_size=1) \n",
    "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
    "    splitted_lengths_batch = lengths.split(split_size=1)\n",
    "\n",
    "    training_data_tuples = zip(splitted_input_sequence_batch,\n",
    "                               splitted_output_sequence_batch,\n",
    "                               splitted_lengths_batch)\n",
    "\n",
    "    training_data_tuples_sorted = sorted(training_data_tuples,\n",
    "                                         key=lambda p: int(p[2]),\n",
    "                                         reverse=True)\n",
    "\n",
    "    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
    "\n",
    "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
    "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
    "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
    "    \n",
    "    # Here we trim overall data matrix using the size of the longest sequence\n",
    "    input_sequence_batch_sorted = input_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0], :]\n",
    "    output_sequence_batch_sorted = output_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0], :]\n",
    "    \n",
    "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
    "    \n",
    "    # pytorch's api for rnns wants lenghts to be list of ints\n",
    "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
    "    lengths_batch_sorted_list = map(lambda x: int(x), lengths_batch_sorted_list)\n",
    "    \n",
    "    return input_sequence_batch_transposed, output_sequence_batch_sorted, list(lengths_batch_sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = NotesGenerationDataset('./MIDI_Files/songs', longest_sequence_length=50)\n",
    "\n",
    "trainset_loader = data.DataLoader(trainset, batch_size=5,\n",
    "                                              shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(233, 46)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(trainset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valset = Dataset_validation(validation_list)\n",
    "\n",
    "valset_loader = data.DataLoader(valset, batch_size=5, shuffle=False, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "169"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout = 0.2)\n",
    "        \n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
    "        batch_size = input_sequences.shape[1]\n",
    "\n",
    "        notes_encoded = self.notes_encoder(input_sequences)\n",
    "        notes_encoded_rolled = notes_encoded.permute(1,2,0).contiguous()\n",
    "        notes_encoded_norm = self.bn(notes_encoded_rolled)\n",
    "        notes_encoded_norm_drop = nn.Dropout(0.25)(notes_encoded_norm)\n",
    "        notes_encoded_complete = notes_encoded_norm_drop.permute(2,0,1)\n",
    "        \n",
    "        # Here we run rnns only on non-padded regions of the batch\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(notes_encoded_complete, input_sequences_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        \n",
    "        outputs_norm = self.bn(outputs.permute(1,2,0).contiguous())\n",
    "        outputs_drop = nn.Dropout(0.1)(outputs_norm)\n",
    "        logits = self.logits_fc(outputs_drop.permute(2,0,1))\n",
    "        logits = logits.transpose(0, 1).contiguous()\n",
    "        \n",
    "        neg_logits = (1 - logits)\n",
    "        \n",
    "        # Since the BCE loss doesn't support masking, we use the crossentropy\n",
    "        binary_logits = torch.stack((logits, neg_logits), dim=3).contiguous()\n",
    "        logits_flatten = binary_logits.view(-1, 2)\n",
    "        return logits_flatten, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marika/Documents/GitHub/EDM_Music/music_venv/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_val = nn.CrossEntropyLoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = 1.0\n",
    "epochs_number = 10\n",
    "sample_history = []\n",
    "best_val_loss = float(\"inf\")\n",
    "layers = list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=88, out_features=512, bias=True),\n",
       " BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " LSTM(512, 512, num_layers=2, dropout=0.2),\n",
       " Linear(in_features=512, out_features=88, bias=True)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrfinder(start, end, model, trainset_loader, epochs=20):\n",
    "    model.train() # into training mode\n",
    "    lrs = np.linspace(start, end, epochs*len(trainset_loader))    \n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters,start)\n",
    "    loss_list = []\n",
    "    ctr = 0\n",
    "    \n",
    "    for epoch_number in range(epochs):\n",
    "        epoch_loss = []\n",
    "        for batch in trainset_loader:\n",
    "            optimizer.param_groups[0]['lr'] = lrs[ctr]\n",
    "            ctr = ctr+1\n",
    "\n",
    "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1) )\n",
    "\n",
    "            input_sequences_batch_var = Variable( input_sequences_batch )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "            loss = criterion(logits, output_sequences_batch_var)\n",
    "            loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "        if epoch_number%5 == 0:\n",
    "            print('Epoch %d' % epoch_number)\n",
    "    plt.plot(lrs, loss_list)\n",
    "    return lrs, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model):\n",
    "    model.eval()\n",
    "    full_val_loss = 0.0\n",
    "    overall_sequence_length = 0.0\n",
    "\n",
    "    for batch in valset_loader:\n",
    "\n",
    "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "        output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1) )\n",
    "\n",
    "        input_sequences_batch_var = Variable( input_sequences_batch )\n",
    "\n",
    "        logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "        loss = criterion_val(logits, output_sequences_batch_var)\n",
    "\n",
    "        full_val_loss += loss.item()\n",
    "        overall_sequence_length += sum(sequences_lengths)\n",
    "\n",
    "    return full_val_loss / (overall_sequence_length * 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "rnn = model\n",
    "# lrs, losses = lrfinder(1e-4, 1e-1, rnn, trainset_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, lrs_triangular, epochs_number=10, wd=0.0, best_val_loss=float(\"inf\")):\n",
    "    loss_list = []\n",
    "    val_list =[]\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lrs_triangular[0], weight_decay=wd)\n",
    "    ctr = 0\n",
    "    \n",
    "    for epoch_number in range(epochs_number):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        for batch in trainset_loader:\n",
    "#             try: \n",
    "#                 optimizer.param_groups[0]['lr'] = lrs_triangular[ctr]\n",
    "#             except IndexError: pass\n",
    "#             ctr+=1\n",
    "\n",
    "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1) )\n",
    "\n",
    "            input_sequences_batch_var = Variable( input_sequences_batch )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "            loss = criterion(logits, output_sequences_batch_var)\n",
    "            loss_list.append(loss.item())\n",
    "            epoch_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        current_trn_epoch = sum(epoch_loss)/len(trainset_loader)\n",
    "        current_val_loss = validate(model)\n",
    "        \n",
    "#         if epoch_number%5 == 0:\n",
    "        print('Training Loss: Epoch:',epoch_number,':', current_trn_epoch)   \n",
    "#         if epoch_number%5 == 0:\n",
    "        print('Validation Loss: Epoch:',epoch_number,':', current_val_loss)\n",
    "        print('')\n",
    "\n",
    "        val_list.append(current_val_loss)\n",
    "\n",
    "        if current_val_loss < best_val_loss:\n",
    "\n",
    "            torch.save(model.state_dict(), 'music_edm_new.pth')\n",
    "            best_val_loss = current_val_loss\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:34: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: Epoch: 0 : 1.0091112942799279\n",
      "Validation Loss: Epoch: 0 : 0.7219924238922135\n",
      "\n",
      "Training Loss: Epoch: 1 : 0.21094281760894734\n",
      "Validation Loss: Epoch: 1 : 0.11124950094977139\n",
      "\n",
      "Training Loss: Epoch: 2 : 0.09160818758866061\n",
      "Validation Loss: Epoch: 2 : 0.08551930116157881\n",
      "\n",
      "Training Loss: Epoch: 3 : 0.0853094006686107\n",
      "Validation Loss: Epoch: 3 : 0.076239292746016\n",
      "\n",
      "Training Loss: Epoch: 4 : 0.07974411927811477\n",
      "Validation Loss: Epoch: 4 : 0.07304424152507649\n",
      "\n",
      "Training Loss: Epoch: 5 : 0.07522562308155972\n",
      "Validation Loss: Epoch: 5 : 0.07387430142817669\n",
      "\n",
      "Training Loss: Epoch: 6 : 0.07586992774968562\n",
      "Validation Loss: Epoch: 6 : 0.07026481657146189\n",
      "\n",
      "Training Loss: Epoch: 7 : 0.07309037276908108\n",
      "Validation Loss: Epoch: 7 : 0.06711740790290689\n",
      "\n",
      "Training Loss: Epoch: 8 : 0.07252027134856452\n",
      "Validation Loss: Epoch: 8 : 0.06709874654342052\n",
      "\n",
      "Training Loss: Epoch: 9 : 0.07054679128138916\n",
      "Validation Loss: Epoch: 9 : 0.06864049245619915\n",
      "\n",
      "Training Loss: Epoch: 10 : 0.06966095830759277\n",
      "Validation Loss: Epoch: 10 : 0.0652856014389194\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "best_val_loss = train_model(rnn, [1e-3], epochs_number=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(sample_length=4, temperature=1, starting_sequence=None):\n",
    "\n",
    "    if starting_sequence is None:\n",
    "                \n",
    "        current_sequence_input = torch.zeros(1, 1, 88)\n",
    "        current_sequence_input[0, 0, 40] = 1\n",
    "        current_sequence_input[0, 0, 50] = 0\n",
    "        current_sequence_input[0, 0, 56] = 0\n",
    "        current_sequence_input = Variable(current_sequence_input)\n",
    "    \n",
    "    else:\n",
    "        current_sequence_input = Variable(starting_sequence)\n",
    "    final_output_sequence = [current_sequence_input.data.squeeze(1)]\n",
    "    \n",
    "    hidden = None    \n",
    "\n",
    "    for i in range(sample_length):\n",
    "\n",
    "        output, hidden = rnn(current_sequence_input, [1], hidden)\n",
    "\n",
    "        probabilities = nn.functional.softmax(output.div(temperature), dim=1)\n",
    "\n",
    "        current_sequence_input = torch.multinomial(probabilities.data, 1).squeeze().unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        current_sequence_input = Variable(current_sequence_input.float())\n",
    "\n",
    "        final_output_sequence.append(current_sequence_input.data.squeeze(1))\n",
    "\n",
    "    sampled_sequence = torch.cat(final_output_sequence, dim=0).numpy()\n",
    "    \n",
    "    return sampled_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1083425c0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = sample_from_piano_rnn(sample_length=80, temperature=0.8).transpose()\n",
    "io.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "midiwrite('sample2.mid', sample.transpose(), dt=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='sample2.mid' target='_blank'>sample2.mid</a><br>"
      ],
      "text/plain": [
       "/Users/marika/Documents/GitHub/EDM_Music/sample2.mid"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('sample2.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_venv",
   "language": "python",
   "name": "music_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
