{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import random\n",
    "PATH = '/Users/marika/Documents/Github/EDM_Music'\n",
    "sys.path.append(os.path.join(PATH, 'midi'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.utils.data as data\n",
    "\n",
    "import pretty_midi\n",
    "from midi_utils import midiread, midiwrite\n",
    "from matplotlib import pyplot as plt\n",
    "import skimage.io as io\n",
    "from IPython.display import FileLink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "pth = Path('/Users/marika/Documents/Github/EDM_Music/Lead-Sheet-Dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.utils.data as data\n",
    "\n",
    "\n",
    "def midi_filename_to_piano_roll(midi_filename):\n",
    "    \n",
    "    midi_data = midiread(midi_filename, dt=0.2)\n",
    "    \n",
    "    piano_roll = midi_data.piano_roll.T\n",
    "    \n",
    "    # Binarize the pressed notes \n",
    "    piano_roll[piano_roll > 0] = 1  # no need as unique values are 0 and 1\n",
    "    \n",
    "    return piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pad_piano_roll(piano_roll, max_length=132333, pad_value=0):\n",
    "    \"\"\"\n",
    "    padding 0 at the beginning of sequence\n",
    "    \n",
    "    \"\"\"\n",
    "    # We hardcode 88 -- because we will always use only\n",
    "    # 88 pitches\n",
    "    \n",
    "    original_piano_roll_length = piano_roll.shape[1]\n",
    "    \n",
    "    padded_piano_roll = np.zeros((88, max_length))\n",
    "    padded_piano_roll[:] = pad_value\n",
    "    \n",
    "    padded_piano_roll[:, :original_piano_roll_length] = piano_roll  # keep 0 padding at begin\n",
    "\n",
    "    return padded_piano_roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# midi_folders = os.listdir('./Lead-Sheet-Dataset/datasets/pianoroll/s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import shutil\n",
    "# midi_filenames = []\n",
    "# base_dir = 'MIDI_Files/nokey'\n",
    "# for i in midi_folders: \n",
    "#     if i!='.DS_Store':\n",
    "#         artist_folder = os.listdir('./Lead-Sheet-Dataset/datasets/pianoroll/s/' + str(i) + '/')\n",
    "#         for j in artist_folder: \n",
    "#             if j!='.DS_Store':\n",
    "#                 folder = os.listdir('./Lead-Sheet-Dataset/datasets/pianoroll/s/' + str(i) + '/' + str(j))\n",
    "#                 for f in folder:\n",
    "#                     if '.mid' in f and 'nokey' in f:\n",
    "#                         source = './Lead-Sheet-Dataset/datasets/pianoroll/s/'+ str(i) + '/' + str(j) + \"/\"+ str(f) \n",
    "                        \n",
    "#                         destination = os.path.join(base_dir, str(i) + '_' + str(j) + '_' + f)\n",
    "                        \n",
    "#                         shutil.copyfile(source, destination)\n",
    "                    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#midi_chorus = []\n",
    "midi_songs = os.listdir('./MIDI_Files/songs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# chorus_path = './MIDI_Files/chorus'\n",
    "# os.mkdir(chorus_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in midi_songs:\n",
    "#     if i!='.DS_Store':\n",
    "#         if 'chorus_key.mid' in i:\n",
    "#             source = './MIDI_Files/songs/' + str(i)\n",
    "#             destination = os.path.join(chorus_path, i)\n",
    "#             shutil.copyfile(source, destination)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# verse_path = './MIDI_Files/verse'\n",
    "# os.mkdir(verse_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in midi_songs:\n",
    "#     if i!='.DS_Store':\n",
    "#         if 'verse_key.mid' in i:\n",
    "#             source = './MIDI_Files/songs/' + str(i)\n",
    "#             destination = os.path.join(verse_path, i)\n",
    "#             shutil.copyfile(source, destination)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# instr_path = './MIDI_Files/intrum'\n",
    "# os.mkdir(instr_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for i in midi_songs:\n",
    "#     if i!='.DS_Store':\n",
    "#         if 'instrumental_key.mid' in i:\n",
    "#             source = './MIDI_Files/songs/' + str(i)\n",
    "#             destination = os.path.join(instr_path, i)\n",
    "#             shutil.copyfile(source, destination)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.remove('./MIDI_Files/songs/.DS_Store')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_folder_path = './MIDI_Files/songs/'\n",
    "midi_filenames = os.listdir(midi_folder_path)\n",
    "midi_full_filenames = [os.path.join(midi_folder_path, filename) for filename in midi_filenames]\n",
    "sequences_lengths = [midi_filename_to_piano_roll(filename).shape[1] for filename in midi_full_filenames]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19, 450, 77.0, 97.56590909090909)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stats of sequence lengths \n",
    "min(sequences_lengths), max(sequences_lengths), np.median(sequences_lengths), np.mean(sequences_lengths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making validation slices of sequence length 50\n",
    "\n",
    "val_slices=[]\n",
    "\n",
    "for k in range(len(midi_filenames)):\n",
    "    piano_roll = midi_filename_to_piano_roll(midi_full_filenames[k])\n",
    "    \n",
    "    for i in range(piano_roll.shape[1]):\n",
    "        if i%50 == 0:\n",
    "            #print ('here')\n",
    "            try:\n",
    "                val_slices.append(piano_roll[:,i:i+51])\n",
    "            except IndexError:\n",
    "                pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validation_list = [v for v in val_slices if v.shape[1] == 51]\n",
    "len(validation_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NotesGenerationDataset(data.Dataset):\n",
    "    \n",
    "    def __init__(self, midi_folder_path, longest_sequence_length=50):\n",
    "        \n",
    "        self.midi_folder_path = midi_folder_path\n",
    "        \n",
    "        midi_filenames = os.listdir(midi_folder_path)\n",
    "        \n",
    "        self.longest_sequence_length = longest_sequence_length\n",
    "        \n",
    "        midi_full_filenames = [os.path.join(midi_folder_path, filename) for filename in midi_filenames]\n",
    "        \n",
    "        self.midi_full_filenames = list(midi_full_filenames)\n",
    "        \n",
    "        if longest_sequence_length is None:            \n",
    "            self.update_the_max_length()\n",
    "    \n",
    "    \n",
    "    def update_the_max_length(self):\n",
    "        \"\"\"Recomputes the longest sequence constant of the dataset.\n",
    "\n",
    "        Reads all the midi files from the midi folder and finds the max\n",
    "        length.\n",
    "        \"\"\"\n",
    "        \n",
    "        sequences_lengths = [midi_filename_to_piano_roll(filename).shape[1] for filename in self.midi_full_filenames]\n",
    "        \n",
    "        max_length = max(sequences_lengths)\n",
    "        \n",
    "        self.longest_sequence_length = max_length\n",
    "                \n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.midi_full_filenames)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        \n",
    "        midi_full_filename = self.midi_full_filenames[index]\n",
    "        piano_roll = midi_filename_to_piano_roll(midi_full_filename)\n",
    "        \n",
    "        # -1 because we will shift it\n",
    "        sequence_length = piano_roll.shape[1] - 1\n",
    "        \n",
    "        # median length of songs\n",
    "        if piano_roll.shape[1] > self.longest_sequence_length:\n",
    "                random_strt_idx = np.random.randint(0, piano_roll.shape[1] -\n",
    "                                                    self.longest_sequence_length+3, 1) # +3 just to make sure of gaps at the end\n",
    "                ground_truth_sequence = piano_roll[:, \n",
    "                                            random_strt_idx[0] :\n",
    "                                            random_strt_idx[0]+self.longest_sequence_length]\n",
    "                input_sequence = piano_roll[:,random_strt_idx[0]-1 :\n",
    "                                            random_strt_idx[0]+self.longest_sequence_length-1]\n",
    "\n",
    "        else:\n",
    "            input_sequence = piano_roll[:,:-1]\n",
    "            ground_truth_sequence = piano_roll[:, 1:]        \n",
    "        \n",
    "        \n",
    "        #print (ground_truth_sequence.shape)\n",
    "        # pad sequence so that all of them have the same lenght\n",
    "        # Otherwise the batching won't work\n",
    "        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length) \n",
    "        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,\n",
    "                                                      max_length=self.longest_sequence_length,\n",
    "                                                      pad_value=0)\n",
    "                \n",
    "        input_sequence_padded = input_sequence_padded.transpose()\n",
    "        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n",
    "        \n",
    "        return (torch.FloatTensor(input_sequence_padded),\n",
    "                torch.LongTensor(ground_truth_sequence_padded),\n",
    "                torch.LongTensor([self.longest_sequence_length]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Dataset_validation(data.Dataset):\n",
    "    \n",
    "    def __init__(self, val_list, longest_sequence_length=50):\n",
    "        \n",
    "        self.val_list = val_list\n",
    "        self.longest_sequence_length = longest_sequence_length\n",
    "    \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.val_list)\n",
    "    \n",
    "    def __getitem__(self, index): \n",
    "        piano_roll = self.val_list[index]\n",
    "        input_sequence = piano_roll[:,:-1]\n",
    "        ground_truth_sequence = piano_roll[:, 1:]        \n",
    "   \n",
    "        # pad sequence so that all of them have the same lenght\n",
    "        # Otherwise the batching won't work\n",
    "        input_sequence_padded = pad_piano_roll(input_sequence, max_length=self.longest_sequence_length) \n",
    "        ground_truth_sequence_padded = pad_piano_roll(ground_truth_sequence,\n",
    "                                                      max_length=self.longest_sequence_length,\n",
    "                                                      pad_value=0)\n",
    "                \n",
    "        input_sequence_padded = input_sequence_padded.transpose()\n",
    "        ground_truth_sequence_padded = ground_truth_sequence_padded.transpose()\n",
    "        \n",
    "        return (torch.FloatTensor(input_sequence_padded),\n",
    "                torch.LongTensor(ground_truth_sequence_padded),\n",
    "                torch.LongTensor([self.longest_sequence_length]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def post_process_sequence_batch(batch_tuple):\n",
    "    \n",
    "    input_sequences, output_sequences, lengths = batch_tuple\n",
    "    \n",
    "    splitted_input_sequence_batch = input_sequences.split(split_size=1) \n",
    "    #print (splitted_input_sentence_batch.shape)\n",
    "    splitted_output_sequence_batch = output_sequences.split(split_size=1)\n",
    "    splitted_lengths_batch = lengths.split(split_size=1)\n",
    "\n",
    "    training_data_tuples = zip(splitted_input_sequence_batch,\n",
    "                               splitted_output_sequence_batch,\n",
    "                               splitted_lengths_batch)\n",
    "\n",
    "    training_data_tuples_sorted = sorted(training_data_tuples,\n",
    "                                         key=lambda p: int(p[2]),\n",
    "                                         reverse=True)\n",
    "\n",
    "    splitted_input_sequence_batch, splitted_output_sequence_batch, splitted_lengths_batch = zip(*training_data_tuples_sorted)\n",
    "\n",
    "    input_sequence_batch_sorted = torch.cat(splitted_input_sequence_batch)\n",
    "    output_sequence_batch_sorted = torch.cat(splitted_output_sequence_batch)\n",
    "    lengths_batch_sorted = torch.cat(splitted_lengths_batch)\n",
    "    \n",
    "    # Here we trim overall data matrix using the size of the longest sequence\n",
    "    input_sequence_batch_sorted = input_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0], :]\n",
    "    output_sequence_batch_sorted = output_sequence_batch_sorted[:, :lengths_batch_sorted[0, 0], :]\n",
    "    \n",
    "    input_sequence_batch_transposed = input_sequence_batch_sorted.transpose(0, 1)\n",
    "    \n",
    "    # pytorch's api for rnns wants lenghts to be list of ints\n",
    "    lengths_batch_sorted_list = list(lengths_batch_sorted)\n",
    "    lengths_batch_sorted_list = map(lambda x: int(x), lengths_batch_sorted_list)\n",
    "    \n",
    "    return input_sequence_batch_transposed, output_sequence_batch_sorted, list(lengths_batch_sorted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "trainset = NotesGenerationDataset('./MIDI_Files/songs', longest_sequence_length=50)\n",
    "\n",
    "trainset_loader = data.DataLoader(trainset, batch_size=5,\n",
    "                                              shuffle=True, num_workers=4, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(440, 88)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainset), len(trainset_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         ...,\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
       "         [0., 0., 0.,  ..., 0., 0., 0.]]), tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0],\n",
       "         [0, 0, 0,  ..., 0, 0, 0]]), tensor([50]))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainset[1][2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "valset = Dataset_validation(validation_list)\n",
    "\n",
    "valset_loader = data.DataLoader(valset, batch_size=5, shuffle=False, num_workers=4, drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "597"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_classes, n_layers=2):\n",
    "        \n",
    "        super(RNN, self).__init__()\n",
    "        \n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_classes = num_classes\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.notes_encoder = nn.Linear(in_features=input_size, out_features=hidden_size)\n",
    "        \n",
    "        self.bn = nn.BatchNorm1d(hidden_size)\n",
    "        \n",
    "        self.lstm = nn.LSTM(hidden_size, hidden_size, n_layers, dropout = 0.2)\n",
    "        \n",
    "        self.logits_fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    \n",
    "    def forward(self, input_sequences, input_sequences_lengths, hidden=None):\n",
    "        batch_size = input_sequences.shape[1]\n",
    "\n",
    "        notes_encoded = self.notes_encoder(input_sequences)\n",
    "        notes_encoded_rolled = notes_encoded.permute(1,2,0).contiguous()\n",
    "        notes_encoded_norm = self.bn(notes_encoded_rolled)\n",
    "        notes_encoded_norm_drop = nn.Dropout(0.25)(notes_encoded_norm)\n",
    "        notes_encoded_complete = notes_encoded_norm_drop.permute(2,0,1)\n",
    "        \n",
    "        # Here we run rnns only on non-padded regions of the batch\n",
    "        packed = torch.nn.utils.rnn.pack_padded_sequence(notes_encoded_complete, input_sequences_lengths)\n",
    "        outputs, hidden = self.lstm(packed, hidden)\n",
    "        outputs, output_lengths = torch.nn.utils.rnn.pad_packed_sequence(outputs) # unpack (back to padded)\n",
    "        \n",
    "        outputs_norm = self.bn(outputs.permute(1,2,0).contiguous())\n",
    "        outputs_drop = nn.Dropout(0.1)(outputs_norm)\n",
    "        logits = self.logits_fc(outputs_drop.permute(2,0,1))\n",
    "        logits = logits.transpose(0, 1).contiguous()\n",
    "        \n",
    "        neg_logits = (1 - logits)\n",
    "        \n",
    "        # Since the BCE loss doesn't support masking, we use the crossentropy\n",
    "        binary_logits = torch.stack((logits, neg_logits), dim=3).contiguous()\n",
    "        logits_flatten = binary_logits.view(-1, 2)\n",
    "        return logits_flatten, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/marika/Documents/GitHub/EDM_Music/music_venv/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    }
   ],
   "source": [
    "model = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "criterion_val = nn.CrossEntropyLoss(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "clip = 1.0\n",
    "epochs_number = 10\n",
    "sample_history = []\n",
    "best_val_loss = float(\"inf\")\n",
    "layers = list(model.children())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Linear(in_features=88, out_features=512, bias=True),\n",
       " BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n",
       " LSTM(512, 512, num_layers=2, dropout=0.2),\n",
       " Linear(in_features=512, out_features=88, bias=True)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def lrfinder(start, end, model, trainset_loader, epochs=20):\n",
    "    model.train() # into training mode\n",
    "    lrs = np.linspace(start, end, epochs*len(trainset_loader))    \n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters,start)\n",
    "    loss_list = []\n",
    "    ctr = 0\n",
    "    \n",
    "    for epoch_number in range(epochs):\n",
    "        epoch_loss = []\n",
    "        for batch in trainset_loader:\n",
    "            optimizer.param_groups[0]['lr'] = lrs[ctr]\n",
    "            ctr = ctr+1\n",
    "\n",
    "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "            \n",
    "\n",
    "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1) )\n",
    "\n",
    "            input_sequences_batch_var = Variable( input_sequences_batch )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "            loss = criterion(logits, output_sequences_batch_var)\n",
    "            loss_list.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(rnn.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "        if epoch_number%5 == 0:\n",
    "            print('Epoch %d' % epoch_number)\n",
    "    plt.plot(lrs, loss_list)\n",
    "    return lrs, loss_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def validate(model):\n",
    "    model.eval()\n",
    "    full_val_loss = 0.0\n",
    "    overall_sequence_length = 0.0\n",
    "\n",
    "    for batch in valset_loader:\n",
    "\n",
    "        post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "        input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "\n",
    "        output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1) )\n",
    "\n",
    "        input_sequences_batch_var = Variable( input_sequences_batch )\n",
    "\n",
    "        logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "\n",
    "        loss = criterion_val(logits, output_sequences_batch_var)\n",
    "\n",
    "        full_val_loss += loss.item()\n",
    "        overall_sequence_length += sum(sequences_lengths)\n",
    "\n",
    "    return full_val_loss / (overall_sequence_length * 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rnn = RNN(input_size=88, hidden_size=512, num_classes=88)\n",
    "rnn = model\n",
    "# lrs, losses = lrfinder(1e-4, 1e-1, rnn, trainset_loader, epochs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, lrs_triangular, epochs_number=10, wd=0.0, best_val_loss=float(\"inf\")):\n",
    "    loss_list = []\n",
    "    val_list =[]\n",
    "    parameters = filter(lambda p: p.requires_grad, model.parameters()) # get all parameters which need grad\n",
    "    optimizer = torch.optim.Adam(parameters, lr=lrs_triangular[0], weight_decay=wd)\n",
    "    ctr = 0\n",
    "    \n",
    "    for epoch_number in range(epochs_number):\n",
    "        model.train()\n",
    "        epoch_loss = []\n",
    "        for batch in trainset_loader:\n",
    "#             try: \n",
    "#                 optimizer.param_groups[0]['lr'] = lrs_triangular[ctr]\n",
    "#             except IndexError: pass\n",
    "#             ctr+=1\n",
    "\n",
    "            post_processed_batch_tuple = post_process_sequence_batch(batch)\n",
    "\n",
    "            input_sequences_batch, output_sequences_batch, sequences_lengths = post_processed_batch_tuple\n",
    "        \n",
    "            #print (input_sequences_batch.shape)\n",
    "           # print (output_sequences_batch.shape)\n",
    "\n",
    "            output_sequences_batch_var =  Variable( output_sequences_batch.contiguous().view(-1) )\n",
    "            #print (output_sequences_batch_var.shape[0])\n",
    "\n",
    "            input_sequences_batch_var = Variable( input_sequences_batch )\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(input_sequences_batch_var, sequences_lengths)\n",
    "            #print (logits.shape)\n",
    "\n",
    "            loss = criterion(logits, output_sequences_batch_var)\n",
    "            loss_list.append(loss.item())\n",
    "            epoch_loss.append(loss.item())\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm(model.parameters(), clip)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        current_trn_epoch = sum(epoch_loss)/len(trainset_loader)\n",
    "        current_val_loss = validate(model)\n",
    "        \n",
    "#         if epoch_number%5 == 0:\n",
    "        print('Training Loss: Epoch:',epoch_number,':', current_trn_epoch)   \n",
    "#         if epoch_number%5 == 0:\n",
    "        print('Validation Loss: Epoch:',epoch_number,':', current_val_loss)\n",
    "        print('')\n",
    "\n",
    "        val_list.append(current_val_loss)\n",
    "\n",
    "        if current_val_loss < best_val_loss:\n",
    "\n",
    "            torch.save(model.state_dict(), 'music_edm_new.pth')\n",
    "            best_val_loss = current_val_loss\n",
    "    return best_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python3/3.6.4_2/Frameworks/Python.framework/Versions/3.6/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: torch.nn.utils.clip_grad_norm is now deprecated in favor of torch.nn.utils.clip_grad_norm_.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Loss: Epoch: 0 : 0.9553556083278223\n",
      "Validation Loss: Epoch: 0 : 0.3608921537694093\n",
      "\n",
      "Training Loss: Epoch: 1 : 0.16907984983514657\n",
      "Validation Loss: Epoch: 1 : 0.09937680184759469\n",
      "\n",
      "Training Loss: Epoch: 2 : 0.09001649128781124\n",
      "Validation Loss: Epoch: 2 : 0.08405253949839901\n",
      "\n",
      "Training Loss: Epoch: 3 : 0.07930593112144958\n",
      "Validation Loss: Epoch: 3 : 0.07657569384527693\n",
      "\n",
      "Training Loss: Epoch: 4 : 0.07391819565302947\n",
      "Validation Loss: Epoch: 4 : 0.07292308153873059\n",
      "\n",
      "Training Loss: Epoch: 5 : 0.07098137680441141\n",
      "Validation Loss: Epoch: 5 : 0.0714598771268526\n",
      "\n",
      "Training Loss: Epoch: 6 : 0.06924333516508341\n",
      "Validation Loss: Epoch: 6 : 0.06922752460533758\n",
      "\n",
      "Training Loss: Epoch: 7 : 0.06828646675090898\n",
      "Validation Loss: Epoch: 7 : 0.06991507860012063\n",
      "\n",
      "Training Loss: Epoch: 8 : 0.06694582468745383\n",
      "Validation Loss: Epoch: 8 : 0.06670604544904328\n",
      "\n",
      "Training Loss: Epoch: 9 : 0.06680117776109414\n",
      "Validation Loss: Epoch: 9 : 0.06497083308145991\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rnn = RNN(input_size=88, hidden_size = 100, num_classes=88, n_layers=2)\n",
    "best_val_loss = train_model(rnn, [1e-3], epochs_number=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sample_from_piano_rnn(sample_length=4, temperature=1, starting_sequence=None):\n",
    "\n",
    "    if starting_sequence is None:\n",
    "                \n",
    "        current_sequence_input = torch.zeros(1, 1, 88)\n",
    "        current_sequence_input[0, 0, 40] = 1\n",
    "        current_sequence_input[0, 0, 50] = 0\n",
    "        current_sequence_input[0, 0, 56] = 0\n",
    "        current_sequence_input = Variable(current_sequence_input)\n",
    "    \n",
    "    else:\n",
    "        current_sequence_input = Variable(starting_sequence)\n",
    "    final_output_sequence = [current_sequence_input.data.squeeze(1)]\n",
    "    \n",
    "    hidden = None    \n",
    "\n",
    "    for i in range(sample_length):\n",
    "\n",
    "        output, hidden = rnn(current_sequence_input, [1], hidden)\n",
    "\n",
    "        probabilities = nn.functional.softmax(output.div(temperature), dim=1)\n",
    "\n",
    "        current_sequence_input = torch.multinomial(probabilities.data, 1).squeeze().unsqueeze(0).unsqueeze(1)\n",
    "\n",
    "        current_sequence_input = Variable(current_sequence_input.float())\n",
    "\n",
    "        final_output_sequence.append(current_sequence_input.data.squeeze(1))\n",
    "\n",
    "    sampled_sequence = torch.cat(final_output_sequence, dim=0).numpy()\n",
    "    \n",
    "    return sampled_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1230aa1d0>"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = sample_from_piano_rnn(sample_length=80, temperature=0.8).transpose()\n",
    "io.imshow(sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "midiwrite('sample_Exp5.mid', sample.transpose(), dt=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='sample_Exp1.mid' target='_blank'>sample_Exp1.mid</a><br>"
      ],
      "text/plain": [
       "/Users/marika/Documents/GitHub/EDM_Music/sample_Exp1.mid"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FileLink('sample_Exp1.mid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "music_venv",
   "language": "python",
   "name": "music_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
