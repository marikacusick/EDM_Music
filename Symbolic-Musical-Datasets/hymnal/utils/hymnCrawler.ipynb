{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import string\n",
    "import random\n",
    "from lxml import etree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class HymnCraler():\n",
    "    BASE_URL = 'https://www.hymnal.net'\n",
    "    \n",
    "    def __init__(self, sleep_time = 0.1, log=True):\n",
    "        self.sleep_time =sleep_time\n",
    "        self.meta_category = {'classic': self.BASE_URL + '/en/song-index/h', \n",
    "            'new_tunes': self.BASE_URL + '/en/song-index/nt', \n",
    "            'new_songs': self.BASE_URL + '/en/song-index/ns', \n",
    "            'children': self.BASE_URL + '/en/song-index/c'}\n",
    "        \n",
    "        self.log = log\n",
    "        self.metadata = None           \n",
    "\n",
    "    def _request_url(self, url, doctype='html'):\n",
    "        response = requests.get(url)\n",
    "        if doctype =='html':\n",
    "            soup = BeautifulSoup(response.text, 'html.parser')\n",
    "            return soup\n",
    "        elif  doctype =='content':\n",
    "            return response.content\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    def _log_print(self, log):\n",
    "        print(log)\n",
    "        if self.log:\n",
    "            with open(\"log.txt\", \"a\") as f:\n",
    "                print(log, file=f)\n",
    "\n",
    "    def fetch_page_list(self, url):\n",
    "        soup = self._request_url(url)\n",
    "        tag_list = soup.find_all('div', {'class':'list-group'})[0].find_all('a', {'class':'list-group-item'})\n",
    "        return [t['href'] for t in tag_list]\n",
    "\n",
    "    def fetch_category_list(self, url):\n",
    "        song_list = []\n",
    "        soup = self._request_url(url)\n",
    "        try:\n",
    "            tag_list = soup.find_all('div', {'class':'letters'})[0].findAll('a')\n",
    "            alphabet_list = [t.text for t in tag_list]\n",
    "            for ch in alphabet_list:\n",
    "                page_url = url+ '/' + ch\n",
    "                self._log_print(page_url)\n",
    "                song_list.extend(self.fetch_page_list(page_url))\n",
    "        except:\n",
    "            self._log_print(url)\n",
    "            song_list.extend(self.fetch_page_list(url))\n",
    "        return song_list\n",
    "\n",
    "    def fetch_song(self, url, song_dir):\n",
    "\n",
    "        soup = self._request_url(url)\n",
    "        \n",
    "        # (url, extension, filename)\n",
    "        data_list = [('/f=mid', '.mid', 'all'), ('/f=mp3', '.mp3', 'audio'), ('/f=tune', '.mid', 'melody'),\n",
    "                     ('/f=ppdf', '.pdf', 'ls_paino'), ('/f=pdf', '.pdf','ls_guitar'), ('/f=gtpdf', '.pdf', 'ls_text')]\n",
    "\n",
    "        # save download files\n",
    "        for d in data_list:\n",
    "            r = requests.get(url+ d[0])\n",
    "\n",
    "            if song_dir:\n",
    "                with open(os.path.join(song_dir,d[2] + d[1]), 'wb') as f:\n",
    "                    f.write(r.content)\n",
    "\n",
    "        # metadata\n",
    "        content_list = []    \n",
    "        tag_list = soup.find_all('div', {'class':'row common-panel'})[0].find_all('div', {'class':'col-xs-7 col-sm-8 no-padding'})\n",
    "\n",
    "        for t in tag_list:\n",
    "            content_list.append(t.text.strip())\n",
    "        label_list = []\n",
    "        tag_list = soup.find_all('div', {'class':'row common-panel'})[0].find_all('label', {'class':'col-xs-5 col-sm-4'})\n",
    "        for t in tag_list:\n",
    "            label_list.append(t.text.replace(':',''))\n",
    "        metadata = dict(zip(label_list, content_list))\n",
    "        \n",
    "        # title\n",
    "        title = soup.find('h1', {'class':\"text-center\"}).text.strip()\n",
    "        metadata['title'] = title\n",
    "        \n",
    "        # lyric table\n",
    "        lyric_xml = soup.find_all('div', {'class':'col-xs-12 lyrics'})[0].find('table')\n",
    "\n",
    "        if song_dir:\n",
    "            with open( os.path.join(song_dir, 'song_metadata.json'), \"w\") as f:\n",
    "                    json.dump(metadata , f)\n",
    "\n",
    "            with open( os.path.join(song_dir, 'lyric.xml'), \"w\", encoding='utf-8') as f:\n",
    "                f.write(str(lyric_xml))\n",
    "\n",
    "        return lyric_xml, metadata\n",
    "\n",
    "    def craw_archive(self, archive_dir='archive'):\n",
    "        metadata = dict()\n",
    "        for k in self.meta_category.keys():\n",
    "            category_url = self.meta_category[k]\n",
    "            metadata[k] = self.fetch_category_list(category_url)\n",
    "\n",
    "        # saving\n",
    "        if archive_dir:\n",
    "            if not os.path.exists(archive_dir):\n",
    "                os.makedirs(archive_dir)\n",
    "\n",
    "            with open(os.path.join(archive_dir, 'archive_metadata.json'), \"w\") as f:\n",
    "                json.dump(metadata , f)\n",
    "\n",
    "        return metadata\n",
    "\n",
    "    def craw_songs(self, metadata, archive_dir='archive'):\n",
    "        count = 0\n",
    "        count_success = 0\n",
    "        for k in list(metadata):\n",
    "            self._log_print('> %s'%k)\n",
    "            category_dir = os.path.join(archive_dir, k)\n",
    "            if not os.path.exists(category_dir):\n",
    "                os.makedirs(category_dir)\n",
    "\n",
    "            song_list = metadata[k]\n",
    "\n",
    "            numOfSongs = len(song_list)\n",
    "            for i in range(numOfSongs):\n",
    "                song_url = self.BASE_URL + song_list[i]\n",
    "                song_id = song_url.split('/')[-1]\n",
    "                self._log_print('    (%d/%d) %s'%(i+1, numOfSongs,  song_url))\n",
    "                song_dir = os.path.join(category_dir, song_id)\n",
    "\n",
    "                if not os.path.exists(song_dir):\n",
    "                    os.makedirs(song_dir)\n",
    "                \n",
    "                try:\n",
    "                    self.fetch_song(song_url, song_dir)\n",
    "                    metadata['err'] = False\n",
    "                    count_success += 1\n",
    "                except:\n",
    "                    self._log_print('error!!')\n",
    "                    metadata['err'] = True\n",
    "                    \n",
    "                count += 1\n",
    "        self._log_print('total: %d songs'%count)\n",
    "        \n",
    "        return metadata\n",
    "    \n",
    "    def reload(self, archive_dir='archive'):\n",
    "        with open(os.path.join(archive_dir, 'archive_metadata.json'), \"r\") as f:\n",
    "            self.metadata =json.load(f)\n",
    "        \n",
    "    def run(self, archive_dir='archive', reload=False):   \n",
    "        \n",
    "        self._log_print(\"=================================================\")\n",
    "        \n",
    "        if not reload:\n",
    "            self.metadata = self.craw_archive(archive_dir=archive_dir)\n",
    "        else:\n",
    "            self.reload(archive_dir=archive_dir)\n",
    "\n",
    "        self.metadata = self.craw_songs(self.metadata, archive_dir=archive_dir)\n",
    "        \n",
    "        with open(os.path.join(archive_dir, 'archive_metadata.json'), \"w\") as f:\n",
    "            json.dump(self.metadata, f)\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> new_songs\n",
      "    (1/1070) https://www.hymnal.net/en/hymn/ns/528\n",
      "    (2/1070) https://www.hymnal.net/en/hymn/ns/595\n",
      "    (3/1070) https://www.hymnal.net/en/hymn/ns/584\n",
      "    (4/1070) https://www.hymnal.net/en/hymn/ns/524\n",
      "    (5/1070) https://www.hymnal.net/en/hymn/ns/501\n",
      "    (6/1070) https://www.hymnal.net/en/hymn/lb/27\n",
      "    (7/1070) https://www.hymnal.net/en/hymn/ns/550\n",
      "    (8/1070) https://www.hymnal.net/en/hymn/ns/378\n",
      "    (9/1070) https://www.hymnal.net/en/hymn/ns/309\n",
      "error!!\n",
      "    (10/1070) https://www.hymnal.net/en/hymn/ns/398\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    hc = HymnCraler()\n",
    "    \n",
    "    s = time.time()\n",
    "    hc.run(reload=True)\n",
    "    e = time.time()\n",
    "    time.strftime(\"\\nElapsed time: %H:%M:%S\", time.gmtime(s-e))\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [mir]",
   "language": "python",
   "name": "Python [mir]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
